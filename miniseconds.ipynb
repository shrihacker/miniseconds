{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a79e23-d530-41da-9e49-3d381d510856",
   "metadata": {},
   "source": [
    "# Imports & Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb33e35-9c9c-4a9b-9699-52ca585a304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import replicate\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "from pytube import YouTube\n",
    "from google.cloud import storage\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set your environment variables. see .env.example\n",
    "load_dotenv() \n",
    "client = storage.Client()\n",
    "o_client = OpenAI()\n",
    "data_dir = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587d7a9-9858-4137-8e12-0a9194f2519d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e591d-b749-48ee-b96b-5e780cafef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = string.ascii_lowercase + string.digits\n",
    "def random_choice():\n",
    "    '''\n",
    "    Return a 8 letter random string\n",
    "    '''\n",
    "    return ''.join(random.choices(alphabet, k=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633e96b-032c-43c4-9d12-e5c44c6af23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bucket(blob_name, file_path):\n",
    "    '''\n",
    "    Upload a file to Google Cloud Storage\n",
    "        Input: blob_name: String, name of Blob on GCS\n",
    "               file_path: String, path to local file to upload\n",
    "        Output: public URL: String, of the uploaded blob\n",
    "    '''\n",
    "    bucket = client.get_bucket(os.getenv('GCS_BUCKET'))\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(file_path)\n",
    "    return blob.public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc05a8-bf1b-4d4a-adf5-8540626bb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(url, fname):\n",
    "    '''\n",
    "    Replicate Deployment call to fetch transcript.\n",
    "    Writes out the transcript JSON and text to files\n",
    "        Input: url: String, of the uploaded blob\n",
    "               fname: String, filename to write out JSON and text files\n",
    "        Output: out: dictionary, output of transcript call with timestamps\n",
    "                concatenated_text: String, full transcript text\n",
    "    '''\n",
    "    deployment = replicate.deployments.get(\"shrihacker/miniseconds-whisperx\")\n",
    "    prediction = deployment.predictions.create(\n",
    "      input={\"audio\": url, \"batch_size\": 16, \"align_output\": True}\n",
    "    )\n",
    "    prediction.wait()\n",
    "    out = prediction.output\n",
    "\n",
    "    # Write out the JSON transcript\n",
    "    fp = open(f\"{data_dir}/{fname}.json\", \"w\")\n",
    "    fp.write(json.dumps(out))\n",
    "\n",
    "    # Write out the text transcript\n",
    "    concatenated_text = \" \".join(item['text'] for item in out)\n",
    "    fp = open(f\"{data_dir}/{fname}.txt\", \"w\")\n",
    "    fp.write(concatenated_text)\n",
    "\n",
    "    return out, concatenated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2b878-0be3-40ac-a642-5854ff97c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube(yt_link, fname):\n",
    "    '''\n",
    "    Download a Youtube Video and write out a unique mp4 and mp3 file\n",
    "    Input: yt_link: String, URL of youtube video\n",
    "               fname: String, filename to write out Audio and Video files\n",
    "    Output: out: audio_path: String path to MP3 audio file \n",
    "                 video_path: String path to MP4 video file\n",
    "    '''\n",
    "    fname = f\"{data_dir}/{fname}.mp4\"\n",
    "    yt = YouTube(yt_link)\n",
    "    video_path = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download(filename=fname)\n",
    "    audio_path = video_path.replace(\".mp4\", \".mp3\")\n",
    "    cmd = f\"ffmpeg -i {video_path} -vn {audio_path} -hide_banner -loglevel error\"\n",
    "    os.system(cmd)\n",
    "    return audio_path, video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1884f-4b60-4bcf-b16d-5db685c0da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Splits a text into chunks with a specified size and overlap.\n",
    "\n",
    "    Input:\n",
    "    text (str): The text to be chunked.\n",
    "    chunk_size (int): The size of each chunk in words.\n",
    "    overlap (int): The number of words to overlap between consecutive chunks.\n",
    "\n",
    "    Output:\n",
    "    list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = ' '.join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap  # Overlap words for the next chunk\n",
    "\n",
    "        # Break the loop if we reach the end of the text\n",
    "        if end >= len(words):\n",
    "            break\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb714ed-c12d-43e6-800f-94b44d4cea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_timestamps(json_data, sentence):\n",
    "    \"\"\"\n",
    "    Finds the start and end timestamps of a specific sentence within a given JSON data structure.\n",
    "\n",
    "    The function searches for the first 20 characters of the sentence at the beginning and the last 20 characters \n",
    "    at the end within the 'text' field of each JSON object. When the start of the sentence is found, it records \n",
    "    the 'start' timestamp. It continues to search for the end of the sentence, and when found, records the 'end' \n",
    "    timestamp of the last word that matches. The search is case-insensitive.\n",
    "\n",
    "    Parameters:\n",
    "    json_data (list of dicts): A list of dictionaries, each containing 'text' and 'words' fields, \n",
    "                               where 'words' is a list of dicts with 'word' and 'end' fields.\n",
    "    sentence (str): The sentence for which the start and end timestamps are to be found.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple (start_time, end_time) where both are timestamps. 'start_time' is the timestamp \n",
    "           when the sentence starts, and 'end_time' is the timestamp when the sentence ends. \n",
    "           Returns (None, None) if the sentence is not found.\n",
    "    \"\"\"\n",
    "    start_time = None\n",
    "    end_time = None\n",
    "    start = sentence[:20]\n",
    "    end = sentence[-20:]\n",
    "    #print(f\"start: {start}, end: {end}\")\n",
    "    for item in json_data:\n",
    "        if start.lower() in item['text'].lower():\n",
    "            #print(item['text'])\n",
    "            start_time = item['start']\n",
    "        if start_time and end.lower() in item['text'].lower():\n",
    "            #print(item['text'])\n",
    "            end_words = end.split(\" \")\n",
    "            end_w = end_words[-1]\n",
    "            item_words = item['words']\n",
    "            for i_w in item_words:\n",
    "                #print(f\"i_w = {i_w}\")\n",
    "                ip_w = i_w['word'].translate(str.maketrans('', '', string.punctuation))\n",
    "                end_w = end_w.translate(str.maketrans('', '', string.punctuation))\n",
    "                #print(f\"ip_w = {ip_w}\")\n",
    "                if end_w == ip_w:\n",
    "                    end_time = i_w['end']\n",
    "                    break\n",
    "        if start_time is not None and end_time is not None:\n",
    "            break\n",
    "\n",
    "    return start_time, end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87069639-c30c-44cc-a732-f776ee76b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a published bestseller NYT Author and Social Media Influencer.\n",
    "Given some long text that is an interview or podcast or transcript of a talk, you find clips that qualify for short form standalone content that will be published online.\n",
    "Clips are defined as a group of sequential sentences that can represent engaging standalone content.\n",
    "Our goal is social media engagement.\n",
    "The maximum length of a clip that qualifies is 20 sentences.\n",
    "You should not alter the text simply find as many clips as you can and return the clipped text, whole and verbatim.\n",
    "Your response should be a JSON list of dictionaries. Make a single key called \"clips\" which is a List of dicts\n",
    "Each dicionary has key 'clip': the whole verbatim clipped text and key 'title': a short soundbite title why the clip is engaging.\n",
    "Example JSON Structure:\n",
    "{\n",
    "    \"clips\" : [ {\"clip\": <verbatim clip that qualifies>, \"title\" : <a short soundbite title>}, \n",
    "                {\"clip\": <verbatim clip that qualifies>, \"title\" : <a short soundbite title>},\n",
    "                ...\n",
    "            ]\n",
    "{\n",
    "\"\"\"\n",
    "def get_quotes(transcript):\n",
    "    \"\"\"\n",
    "    Extracts engaging clips from a given transcript for social media content.\n",
    "\n",
    "    The function utilizes a GPT-4 model to analyze the transcript and extract relevant clips. Each extracted \n",
    "    clip is accompanied by a short, engaging title that encapsulates its essence. The output is structured as \n",
    "    a JSON object with a single key \"clips\", which is a list of dictionaries. Each dictionary contains two keys: \n",
    "    'clip' for the verbatim text of the extracted segment, and 'title' for its engaging title.\n",
    "\n",
    "    Parameters:\n",
    "    transcript (str): The long text input, such as a transcript of a talk, interview, or podcast.\n",
    "\n",
    "    Returns:\n",
    "    str: A JSON string representing a list of extracted clips. Each clip is a dictionary with 'clip' and 'title' keys.\n",
    "    \"\"\"\n",
    "    response = o_client.chat.completions.create(\n",
    "      model=\"gpt-4-1106-preview\",\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": transcript}\n",
    "      ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=4095,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        n = 1\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ace61-1e18-448c-ad7b-8361a5da90ae",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107a570-ea29-47a7-80cc-f6c65d0cf8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Generate a random name, and fetch a Youtube video's MP4 and MP3 with that name\n",
    "fname = random_choice()\n",
    "audio_path, video_path = get_youtube(\"https://youtu.be/9uOMectkCCs?si=PNvQKilpwlxxbLPe\", fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458662d-0cfa-4203-99ee-2f496132763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 : Upload the audio path to the storage bucket and get its transcript\n",
    "url = upload_to_bucket(f\"{fname}.mp3\", audio_path)\n",
    "out, concatenated_text = get_transcript(url, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e379a8-91e2-4b09-9d68-0093426df0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get Quotes from the transcript\n",
    "oai_response = get_quotes(concatenated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10bc5f-8265-421f-bb60-26f17efc80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract the clips\n",
    "clip_dict = json.loads(oai_response)\n",
    "clip_list = clip_dict[\"clips\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9edc1af-78d3-4459-a9ac-79ccdeb5699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a14e3-588f-4800-a152-8c0ea1cfc6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Clip it!\n",
    "count = 0\n",
    "for c in clip_list:\n",
    "    print(f\"{c['title']} : {c['clip']}\")\n",
    "    s_t , e_t = find_timestamps(out, c['clip'])\n",
    "    #print(f\"found {s_t} and {e_t}\\n\\n\")\n",
    "    if s_t and e_t:\n",
    "        cmd = f\"ffmpeg -y -ss {s_t} -to {e_t} -i {video_path}  -c copy {data_dir}/{fname}_clipped_{count}.mp4 -hide_banner -loglevel error\"\n",
    "        os.system(cmd)\n",
    "        count = count+1\n",
    "    else:\n",
    "        print(f\"Couldnt find {c['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f107b-acb5-41c2-8538-bd408988a49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
